{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formula 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "12"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the below codeblock if you want to test logistic regression and/or neural network. \n",
    "\n",
    "For SVM's run the block below the next one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Config for regression/NN\n",
    "regress = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the codeblock below if you want to test SVMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# Config for SVMs\n",
    "regress = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Gather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "12"
    }
   },
   "outputs": [],
   "source": [
    "# Set data directory path\n",
    "data_dir_path = \"../data/\"\n",
    "\n",
    "# Read relevant csv files\n",
    "results = pd.read_csv(data_dir_path + \"results.csv\", na_values=np.nan)\n",
    "driver_standings = pd.read_csv(data_dir_path + \"driver_standings.csv\", na_values=np.nan)\n",
    "constructor_standings = pd.read_csv(\n",
    "    data_dir_path + \"constructor_standings.csv\", na_values=np.nan\n",
    ")\n",
    "raceYears = pd.read_csv(data_dir_path + \"races.csv\", na_values=np.nan)\n",
    "names = pd.read_csv(data_dir_path + \"drivers.csv\", na_values=np.nan)\n",
    "circuits = pd.read_csv(data_dir_path + \"circuits.csv\", na_values=np.nan)\n",
    "\n",
    "# Filter columns\n",
    "raceYears = raceYears[[\"raceId\", \"year\", \"circuitId\"]]\n",
    "raceYears = raceYears[raceYears[\"year\"] >= 2014]  # Only data after 2014\n",
    "results = results[[\"raceId\", \"driverId\", \"constructorId\", \"grid\", \"position\"]]\n",
    "driver_standings = driver_standings[\n",
    "    [\"raceId\", \"driverId\", \"position\", \"wins\", \"points\"]\n",
    "]\n",
    "constructor_standings = constructor_standings[\n",
    "    [\"raceId\", \"constructorId\", \"position\", \"wins\", \"points\"]\n",
    "]\n",
    "names = names[[\"driverId\", \"driverRef\"]]\n",
    "circuits = circuits[[\"circuitId\", \"lat\", \"lng\", \"alt\"]]\n",
    "\n",
    "# Rename because other csvs also have position\n",
    "driver_standings = driver_standings.rename(columns={\"position\": \"driverStanding\"})\n",
    "constructor_standings = constructor_standings.rename(\n",
    "    columns={\n",
    "        \"position\": \"constructorStanding\",\n",
    "        \"wins\": \"constructorWins\",\n",
    "        \"points\": \"constructorPoints\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Merge dataframes\n",
    "race_circuit = pd.merge(raceYears, circuits, on=[\"circuitId\"], how=\"inner\")\n",
    "year_driver_standing = pd.merge(\n",
    "    race_circuit, driver_standings, on=[\"raceId\"], how=\"inner\"\n",
    ")\n",
    "results_driver_standings = pd.merge(\n",
    "    results, year_driver_standing, on=[\"raceId\", \"driverId\"], how=\"inner\"\n",
    ")\n",
    "joined_data = pd.merge(\n",
    "    results_driver_standings,\n",
    "    constructor_standings,\n",
    "    on=[\"raceId\", \"constructorId\"],\n",
    "    how=\"inner\",\n",
    ")\n",
    "joined_data = pd.merge(joined_data, names, on=[\"driverId\"], how=\"inner\")\n",
    "\n",
    "# Drop irrelevant columns\n",
    "joined_data.drop(\n",
    "    columns=[\"raceId\", \"constructorId\", \"driverId\", \"circuitId\"], inplace=True\n",
    ")\n",
    "\n",
    "# Replace proprietary null values with last palce (20 after 2014)\n",
    "joined_data.replace(to_replace=\"\\\\N\", value=20, inplace=True)\n",
    "joined_data[\"position\"] = joined_data[\"position\"].astype(int)\n",
    "\n",
    "# Drop year column\n",
    "joined_data.drop(\"year\", axis=1, inplace=True)\n",
    "\n",
    "# Set feature and target variables\n",
    "X = joined_data[\n",
    "    [\n",
    "        \"grid\",\n",
    "        \"driverStanding\",\n",
    "        \"constructorStanding\",\n",
    "        \"wins\",\n",
    "        \"points\",\n",
    "        \"lat\",\n",
    "        \"lng\",\n",
    "        \"alt\",\n",
    "        \"constructorPoints\",\n",
    "        \"constructorWins\",\n",
    "    ]\n",
    "]\n",
    "Y = joined_data[[\"position\"]]\n",
    "\n",
    "\n",
    "# Divide by train data and test data and shuffle\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "12"
    }
   },
   "outputs": [],
   "source": [
    "azerbaijan_2023 = pd.DataFrame(\n",
    "    {\n",
    "        \"grid\": [3, 1, 6],\n",
    "        \"driverStanding\": [2, 10, 3],\n",
    "        \"constructorStanding\": [1, 4, 2],\n",
    "        \"wins\": [2, 0, 0],\n",
    "        \"points\": [54, 6, 45],\n",
    "        \"lat\": [40.3725, 40.3725, 40.3725],\n",
    "        \"lng\": [49.8533, 49.8533, 49.8533],\n",
    "        \"alt\": [-7, -7, -7],\n",
    "        \"constructorPoints\": [123, 26, 65],\n",
    "        \"constructorWins\": [180, 0, 0],\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "12"
    }
   },
   "outputs": [],
   "source": [
    "def edit_Y(y, regression=True):\n",
    "    \"\"\"Edits Y to be 1 or not 1 for regression or not regression.\n",
    "\n",
    "    Notes:\n",
    "        Pandas DF is passed by reference, so no returns.\n",
    "\n",
    "    Args:\n",
    "        y (pd.DataFrame): Dataframe with column position\n",
    "        regression (bool, optional): True if regression, false for others.\n",
    "    \"\"\"\n",
    "    y.loc[(y[\"position\"] != 1) & (y[\"position\"] != 2) & (y[\"position\"] != 3)] = (\n",
    "        0 if regression else -1\n",
    "    )\n",
    "    ## Uncomment for binary - podium vs not podium\n",
    "    # y.loc[(y[\"position\"] == 2) | (y[\"position\"] == 3)] = 1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Standardise and Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "12"
    }
   },
   "outputs": [],
   "source": [
    "# Convert features to numpy arrays\n",
    "X_train = X_train.to_numpy()\n",
    "X_test = X_test.to_numpy()\n",
    "\n",
    "azerbaijan_2023_np = azerbaijan_2023.to_numpy()\n",
    "\n",
    "# Transform position for regression/NN or SVM\n",
    "edit_Y(Y_train, regression=regress)\n",
    "edit_Y(Y_test, regression=regress)\n",
    "\n",
    "# Convert results to numpy arrays\n",
    "Y_train = Y_train.to_numpy()\n",
    "Y_test = Y_test.to_numpy()\n",
    "\n",
    "# Reshape to 1D array\n",
    "Y_train = Y_train.reshape(Y_train.shape[0])\n",
    "Y_test = Y_test.reshape(Y_test.shape[0])\n",
    "\n",
    "# Scale data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "azerbaijan_2023_np = scaler.transform(azerbaijan_2023_np)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Plotting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "12"
    }
   },
   "outputs": [],
   "source": [
    "def plot(\n",
    "    training_error, testing_error, training_acc, testing_acc, xaxis, xlabel, title=\"\"\n",
    "):\n",
    "    \"\"\"Plots training and testing error and accuracy.\n",
    "\n",
    "    Args:\n",
    "        training_error (list): Training error\n",
    "        testing_error (list): Testing error\n",
    "        training_acc (list): Training accuracy\n",
    "        testing_acc (list): Testing accuracy\n",
    "        xaxis (list): X axis labels\n",
    "        xlabel (str): X axis label\n",
    "        title (str, optional): Title of plot. Defaults to \"\".\n",
    "\n",
    "    Returns:\n",
    "        None. Plots graph.\n",
    "    \"\"\"\n",
    "    # Figure size and title\n",
    "    plt.figure(figsize=(15, 6), dpi=80)\n",
    "    plt.suptitle(title, fontsize=16)\n",
    "\n",
    "    # Plot training and testing error in first subplot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(training_error, label=\"Training Error\")\n",
    "    plt.plot(testing_error, label=\"Testing Error\")\n",
    "    plt.xticks(np.arange(len(xaxis)), xaxis)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(\"Error\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot training and testing accuracy in second subplot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(training_acc, label=\"Training Accuracy\")\n",
    "    plt.plot(testing_acc, label=\"Testing Accuracy\")\n",
    "    plt.xticks(np.arange(len(xaxis)), xaxis)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4A Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def polynomial_logistic_regression(X_train, Y_train, X_test, Y_test, max_degree=6):\n",
    "    \"\"\"Polynomial logistic regression\n",
    "\n",
    "    Args:\n",
    "        X_train (np.array): Training data\n",
    "        Y_train (np.array): Training labels\n",
    "        X_test (np.array): Testing data\n",
    "        Y_test (np.array): Testing labels\n",
    "        max_degree (int, optional): Max degree of polynomial. Defaults to 6.\n",
    "\n",
    "    Returns:\n",
    "        training_error (list): Training errors for each cost\n",
    "        testing_error (list): Testing errors for each cost\n",
    "        training_acc (list): Training accuracies for each cost\n",
    "        testing_acc (list): Testing accuracies for each cost\n",
    "    \"\"\"\n",
    "    # Init logistic regression\n",
    "    _lambda = 0.7\n",
    "    logreg = LogisticRegression(\n",
    "        multi_class=\"multinomial\", max_iter=100_000, penalty=\"l2\", C=1 / _lambda\n",
    "    )\n",
    "\n",
    "    # Init lists for errors and accuracies\n",
    "    training_error, testing_error = [], []\n",
    "    training_acc, testing_acc = [], []\n",
    "\n",
    "    # Loop through degrees\n",
    "    print(\"Polynomial Logistic Regression Score\")\n",
    "    print(\"Degree \\t Test Score \\t Train Score\")\n",
    "    for i in range(1, max_degree + 1):\n",
    "        # Init polynomial features\n",
    "        poly = PolynomialFeatures(degree=i)\n",
    "\n",
    "        # Polynomial fit of training data\n",
    "        X_train_poly = poly.fit_transform(X_train)\n",
    "\n",
    "        # Train model\n",
    "        logreg.fit(X_train_poly, Y_train)\n",
    "\n",
    "        # Polynomial fit of test data\n",
    "        X_test_poly = poly.fit_transform(X_test)\n",
    "\n",
    "        # Get scores\n",
    "        training_score = logreg.score(X_train_poly, Y_train)\n",
    "        testing_score = logreg.score(X_test_poly, Y_test)\n",
    "\n",
    "        # Append accuracy\n",
    "        training_acc.append(training_score)\n",
    "        testing_acc.append(testing_score)\n",
    "\n",
    "        # Append errors\n",
    "        training_error.append(1 - training_score)\n",
    "        testing_error.append(1 - testing_score)\n",
    "\n",
    "        # Fancy print B)\n",
    "        print(f\"{i} \\t {testing_score*100.0:.2f}% \\t {training_score*100.0:.2f}%\")\n",
    "\n",
    "    return training_error, testing_error, training_acc, testing_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "max_degree = 7\n",
    "(\n",
    "    training_error,\n",
    "    testing_error,\n",
    "    training_acc,\n",
    "    testing_acc,\n",
    ") = polynomial_logistic_regression(\n",
    "    X_train, Y_train, X_test, Y_test, max_degree=max_degree\n",
    ")\n",
    "\n",
    "plot(\n",
    "    training_error,\n",
    "    testing_error,\n",
    "    training_acc,\n",
    "    testing_acc,\n",
    "    range(1, max_degree + 1),\n",
    "    \"Degree\",\n",
    "    title=\"Polynomial Logistic Regression\",\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4B SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "def rbf_svm(X_train, Y_train, X_test, Y_test, costs, gamma=0.1):\n",
    "    \"\"\"RBF SVM\n",
    "\n",
    "    Args:\n",
    "        X_train (np.array): Training data\n",
    "        Y_train (np.array): Training labels\n",
    "        X_test (np.array): Testing data\n",
    "        Y_test (np.array): Testing labels\n",
    "        costs (list): The costs to test\n",
    "        gamma (float, optional): Gamma value. Defaults to 0.1.\n",
    "\n",
    "    Returns:\n",
    "        training_error (list): Training errors for each cost\n",
    "        testing_error (list): Testing errors for each cost\n",
    "        training_acc (list): Training accuracies for each cost\n",
    "        testing_acc (list): Testing accuracies for each cost\n",
    "    \"\"\"\n",
    "    training_error, testing_error = [], []\n",
    "    training_acc, testing_acc = [], []\n",
    "\n",
    "    print(\"RBF SVM Score\")\n",
    "    print(\"C \\t Test Score \\t Train Score\")\n",
    "\n",
    "    for c in costs:\n",
    "        svmachine = SVC(kernel=\"rbf\", C=c, gamma=gamma)\n",
    "        svmachine.fit(X_train, Y_train)\n",
    "\n",
    "        ## Uncomment to plot confusion matrix\n",
    "        # predictions = svmachine.predict(X_test)\n",
    "\n",
    "        # cm = confusion_matrix(Y_test, predictions, labels=svmachine.classes_)\n",
    "        # disp = ConfusionMatrixDisplay(\n",
    "        #     confusion_matrix=cm, display_labels=svmachine.classes_\n",
    "        # )\n",
    "        # disp.plot()\n",
    "\n",
    "        # Get scores\n",
    "        training_score = svmachine.score(X_train, Y_train)\n",
    "        testing_score = svmachine.score(X_test, Y_test)\n",
    "\n",
    "        # Append accuracy\n",
    "        training_acc.append(training_score)\n",
    "        testing_acc.append(testing_score)\n",
    "\n",
    "        # Append errors\n",
    "        training_error.append(1 - training_score)\n",
    "        testing_error.append(1 - testing_score)\n",
    "\n",
    "        # Fancy print B)\n",
    "        print(f\"{c} \\t {testing_score*100.0:.2f}% \\t {training_score*100.0:.2f}%\")\n",
    "\n",
    "    return training_error, testing_error, training_acc, testing_acc\n",
    "\n",
    "\n",
    "def polynomial_svm(X_train, Y_train, X_test, Y_test, max_degree=6, c=0.1):\n",
    "    \"\"\"Polynomial Support Vector Machine\n",
    "\n",
    "    Args:\n",
    "        X_train (np.array): Training data\n",
    "        Y_train (np.array): Training labels\n",
    "        X_test (np.array): Testing data\n",
    "        Y_test (np.array): Testing labels\n",
    "        max_degree (int, optional): The maximum degree to test. Defaults to 6.\n",
    "        c (float, optional): The cost to use. Defaults to 0.1.\n",
    "\n",
    "    Returns:\n",
    "        training_error (list): Training errors for each degree\n",
    "        testing_error (list): Testing errors for each degree\n",
    "        training_acc (list): Training accuracies for each degree\n",
    "        testing_acc (list): Testing accuracies for each degree\n",
    "    \"\"\"\n",
    "    training_error, testing_error = [], []\n",
    "    training_acc, testing_acc = [], []\n",
    "\n",
    "    print(\"Polynomial SVM Score\")\n",
    "    print(\"Degree \\t Test Score \\t Train Score\")\n",
    "    for i in range(1, max_degree + 1):\n",
    "        svmachine = SVC(kernel=\"poly\", degree=i, C=c)\n",
    "        svmachine.fit(X_train, Y_train)\n",
    "\n",
    "        ## Uncomment to plot confusion matrix\n",
    "        # predictions = svmachine.predict(X_test)\n",
    "\n",
    "        # cm = confusion_matrix(Y_test, predictions, labels=svmachine.classes_)\n",
    "        # disp = ConfusionMatrixDisplay(\n",
    "        #     confusion_matrix=cm, display_labels=svmachine.classes_\n",
    "        # )\n",
    "        # disp.plot()\n",
    "\n",
    "        # Get scores\n",
    "        training_score = svmachine.score(X_train, Y_train)\n",
    "        testing_score = svmachine.score(X_test, Y_test)\n",
    "\n",
    "        # Append accuracy\n",
    "        training_acc.append(training_score)\n",
    "        testing_acc.append(testing_score)\n",
    "\n",
    "        # Append errors\n",
    "        training_error.append(1 - training_score)\n",
    "        testing_error.append(1 - testing_score)\n",
    "\n",
    "        # Fancy print B)\n",
    "        print(f\"{i} \\t {testing_score*100.0:.2f}% \\t {training_score*100.0:.2f}%\")\n",
    "\n",
    "    return training_error, testing_error, training_acc, testing_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# Set overkill max degree\n",
    "max_degree = 20\n",
    "\n",
    "# Get polynomial logistic regression\n",
    "training_error, testing_error, training_acc, testing_acc = polynomial_svm(\n",
    "    X_train, Y_train, X_test, Y_test, max_degree=max_degree\n",
    ")\n",
    "\n",
    "# Plot\n",
    "plot(\n",
    "    training_error,\n",
    "    testing_error,\n",
    "    training_acc,\n",
    "    testing_acc,\n",
    "    range(1, max_degree + 1),\n",
    "    \"Degree\",\n",
    "    title=\"Polynomial SVM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# Set costs and max degree\n",
    "costs = [1e-4, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3]\n",
    "max_degree = 10\n",
    "\n",
    "print(\"\\033[1m\", \"Polynomial SVM with different costs\", \"\\033[0m\", sep=\"\")\n",
    "for c in costs:\n",
    "    print(\"\\033[1m\", f\"Cost: {c}\", \"\\033[0m\", sep=\"\")\n",
    "    # Get polynomial logistic regression\n",
    "    training_error, testing_error, training_acc, testing_acc = polynomial_svm(\n",
    "        X_train, Y_train, X_test, Y_test, max_degree=max_degree, c=c\n",
    "    )\n",
    "\n",
    "    plot(\n",
    "        training_error,\n",
    "        testing_error,\n",
    "        training_acc,\n",
    "        testing_acc,\n",
    "        range(1, max_degree + 1),\n",
    "        \"Degree\",\n",
    "        title=f\"Polynomial SVM with cost {c}\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# Set costs and max degree\n",
    "costs = [1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3]\n",
    "gammas = [1e-6, 1e-5, 1e-4, 1e-3, 1e-2]\n",
    "\n",
    "print(\"\\033[1m\", \"Polynomial SVM with different Gammas\", \"\\033[0m\", sep=\"\")\n",
    "for gam in gammas:\n",
    "    print(\"\\033[1m\", f\"Gamma: {gam}\", \"\\033[0m\", sep=\"\")\n",
    "    # Get polynomial logistic regression\n",
    "    training_error, testing_error, training_acc, testing_acc = rbf_svm(\n",
    "        X_train, Y_train, X_test, Y_test, costs=costs, gamma=gam\n",
    "    )\n",
    "\n",
    "    plot(\n",
    "        training_error,\n",
    "        testing_error,\n",
    "        training_acc,\n",
    "        testing_acc,\n",
    "        costs,\n",
    "        \"Cost\",\n",
    "        title=f\"RBF with Ɣ {gam}\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# Set costs\n",
    "costs = [1e-4, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3]\n",
    "\n",
    "# Get polynomial logistic regression\n",
    "training_error, testing_error, training_acc, testing_acc = rbf_svm(\n",
    "    X_train, Y_train, X_test, Y_test, costs\n",
    ")\n",
    "\n",
    "# Plot\n",
    "plot(\n",
    "    training_error,\n",
    "    testing_error,\n",
    "    training_acc,\n",
    "    testing_acc,\n",
    "    costs,\n",
    "    \"Cost\",\n",
    "    title=\"RBF SVM\",\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4C Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def nn_model(X_train, Y_train, X_test, Y_test):\n",
    "    \"\"\"Neural Network Model\n",
    "\n",
    "    Args:\n",
    "        X_train (np.array): Training data\n",
    "        Y_train (np.array): Training labels\n",
    "        X_test (np.array): Testing data\n",
    "        Y_test (np.array): Testing labels\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    # Define the model architecture\n",
    "    model = Sequential(\n",
    "        [\n",
    "            Dense(8, activation=\"softmax\", input_shape=(10,)),\n",
    "            Dense(4, activation=\"softmax\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Compile the model with an optimizer, loss function, and metrics\n",
    "    model.compile(\n",
    "        optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    Y_train_one_hot = tf.one_hot(Y_train - 1, depth=4)\n",
    "    Y_test_one_hot = tf.one_hot(Y_test - 1, depth=4)\n",
    "\n",
    "    # Train the model on the training data\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        Y_train_one_hot,\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        validation_data=(X_test, Y_test_one_hot),\n",
    "    )\n",
    "\n",
    "    # Evaluate the model on the test data\n",
    "    loss, accuracy = model.evaluate(X_test, Y_test_one_hot)\n",
    "\n",
    "    finals = model.predict(azerbaijan_2023_np)\n",
    "\n",
    "    print(f\"Test loss: {loss*100:.2f}% \\t Test accuracy: {accuracy*100:.2f}%\")\n",
    "\n",
    "    return finals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "93/93 [==============================] - 1s 3ms/step - loss: 0.2099 - accuracy: 0.0560 - val_loss: 0.1896 - val_accuracy: 0.0499\n",
      "Epoch 2/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.2047 - accuracy: 0.0830 - val_loss: 0.1856 - val_accuracy: 0.0916\n",
      "Epoch 3/100\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2021 - accuracy: 0.2304 - val_loss: 0.1831 - val_accuracy: 0.3113\n",
      "Epoch 4/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.2005 - accuracy: 0.3946 - val_loss: 0.1813 - val_accuracy: 0.4259\n",
      "Epoch 5/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1993 - accuracy: 0.4847 - val_loss: 0.1794 - val_accuracy: 0.5067\n",
      "Epoch 6/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1974 - accuracy: 0.5359 - val_loss: 0.1767 - val_accuracy: 0.5337\n",
      "Epoch 7/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1951 - accuracy: 0.5609 - val_loss: 0.1747 - val_accuracy: 0.5566\n",
      "Epoch 8/100\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1939 - accuracy: 0.5734 - val_loss: 0.1740 - val_accuracy: 0.5728\n",
      "Epoch 9/100\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1930 - accuracy: 0.5875 - val_loss: 0.1735 - val_accuracy: 0.5876\n",
      "Epoch 10/100\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1924 - accuracy: 0.6051 - val_loss: 0.1733 - val_accuracy: 0.6011\n",
      "Epoch 11/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1920 - accuracy: 0.6148 - val_loss: 0.1729 - val_accuracy: 0.6186\n",
      "Epoch 12/100\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1916 - accuracy: 0.6314 - val_loss: 0.1725 - val_accuracy: 0.6402\n",
      "Epoch 13/100\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1913 - accuracy: 0.6486 - val_loss: 0.1721 - val_accuracy: 0.6590\n",
      "Epoch 14/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1909 - accuracy: 0.6664 - val_loss: 0.1717 - val_accuracy: 0.6819\n",
      "Epoch 15/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1905 - accuracy: 0.6830 - val_loss: 0.1713 - val_accuracy: 0.6968\n",
      "Epoch 16/100\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1902 - accuracy: 0.6981 - val_loss: 0.1710 - val_accuracy: 0.7156\n",
      "Epoch 17/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1899 - accuracy: 0.7123 - val_loss: 0.1707 - val_accuracy: 0.7291\n",
      "Epoch 18/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1897 - accuracy: 0.7322 - val_loss: 0.1706 - val_accuracy: 0.7493\n",
      "Epoch 19/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1897 - accuracy: 0.7444 - val_loss: 0.1707 - val_accuracy: 0.7642\n",
      "Epoch 20/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1896 - accuracy: 0.7558 - val_loss: 0.1707 - val_accuracy: 0.7682\n",
      "Epoch 21/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1897 - accuracy: 0.7639 - val_loss: 0.1707 - val_accuracy: 0.7722\n",
      "Epoch 22/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1897 - accuracy: 0.7723 - val_loss: 0.1708 - val_accuracy: 0.7790\n",
      "Epoch 23/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1898 - accuracy: 0.7798 - val_loss: 0.1706 - val_accuracy: 0.7871\n",
      "Epoch 24/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1900 - accuracy: 0.7879 - val_loss: 0.1706 - val_accuracy: 0.7938\n",
      "Epoch 25/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1901 - accuracy: 0.7943 - val_loss: 0.1706 - val_accuracy: 0.7992\n",
      "Epoch 26/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1902 - accuracy: 0.8013 - val_loss: 0.1706 - val_accuracy: 0.8046\n",
      "Epoch 27/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1902 - accuracy: 0.8054 - val_loss: 0.1707 - val_accuracy: 0.8059\n",
      "Epoch 28/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1904 - accuracy: 0.8091 - val_loss: 0.1706 - val_accuracy: 0.8073\n",
      "Epoch 29/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1903 - accuracy: 0.8132 - val_loss: 0.1705 - val_accuracy: 0.8086\n",
      "Epoch 30/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1905 - accuracy: 0.8196 - val_loss: 0.1705 - val_accuracy: 0.8086\n",
      "Epoch 31/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1906 - accuracy: 0.8243 - val_loss: 0.1703 - val_accuracy: 0.8140\n",
      "Epoch 32/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1904 - accuracy: 0.8266 - val_loss: 0.1700 - val_accuracy: 0.8167\n",
      "Epoch 33/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1903 - accuracy: 0.8290 - val_loss: 0.1698 - val_accuracy: 0.8194\n",
      "Epoch 34/100\n",
      "93/93 [==============================] - 0s 3ms/step - loss: 0.1901 - accuracy: 0.8324 - val_loss: 0.1696 - val_accuracy: 0.8194\n",
      "Epoch 35/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1900 - accuracy: 0.8334 - val_loss: 0.1694 - val_accuracy: 0.8221\n",
      "Epoch 36/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1898 - accuracy: 0.8432 - val_loss: 0.1691 - val_accuracy: 0.8908\n",
      "Epoch 37/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1896 - accuracy: 0.8907 - val_loss: 0.1689 - val_accuracy: 0.9151\n",
      "Epoch 38/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1895 - accuracy: 0.8951 - val_loss: 0.1686 - val_accuracy: 0.9164\n",
      "Epoch 39/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1893 - accuracy: 0.8981 - val_loss: 0.1685 - val_accuracy: 0.9164\n",
      "Epoch 40/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1893 - accuracy: 0.8981 - val_loss: 0.1684 - val_accuracy: 0.9164\n",
      "Epoch 41/100\n",
      "93/93 [==============================] - 0s 3ms/step - loss: 0.1892 - accuracy: 0.8981 - val_loss: 0.1682 - val_accuracy: 0.9164\n",
      "Epoch 42/100\n",
      "93/93 [==============================] - 0s 3ms/step - loss: 0.1892 - accuracy: 0.8981 - val_loss: 0.1681 - val_accuracy: 0.9164\n",
      "Epoch 43/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1892 - accuracy: 0.8981 - val_loss: 0.1680 - val_accuracy: 0.9164\n",
      "Epoch 44/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1894 - accuracy: 0.8981 - val_loss: 0.1680 - val_accuracy: 0.9164\n",
      "Epoch 45/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1895 - accuracy: 0.8981 - val_loss: 0.1679 - val_accuracy: 0.9164\n",
      "Epoch 46/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1897 - accuracy: 0.8981 - val_loss: 0.1679 - val_accuracy: 0.9164\n",
      "Epoch 47/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1898 - accuracy: 0.8981 - val_loss: 0.1678 - val_accuracy: 0.9164\n",
      "Epoch 48/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1899 - accuracy: 0.8981 - val_loss: 0.1677 - val_accuracy: 0.9164\n",
      "Epoch 49/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1902 - accuracy: 0.8981 - val_loss: 0.1677 - val_accuracy: 0.9164\n",
      "Epoch 50/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1903 - accuracy: 0.8981 - val_loss: 0.1675 - val_accuracy: 0.9164\n",
      "Epoch 51/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1904 - accuracy: 0.8981 - val_loss: 0.1674 - val_accuracy: 0.9164\n",
      "Epoch 52/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1907 - accuracy: 0.8981 - val_loss: 0.1673 - val_accuracy: 0.9164\n",
      "Epoch 53/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1911 - accuracy: 0.8981 - val_loss: 0.1673 - val_accuracy: 0.9164\n",
      "Epoch 54/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1914 - accuracy: 0.8981 - val_loss: 0.1673 - val_accuracy: 0.9164\n",
      "Epoch 55/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1917 - accuracy: 0.8981 - val_loss: 0.1674 - val_accuracy: 0.9164\n",
      "Epoch 56/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1921 - accuracy: 0.8981 - val_loss: 0.1677 - val_accuracy: 0.9164\n",
      "Epoch 57/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1923 - accuracy: 0.8981 - val_loss: 0.1680 - val_accuracy: 0.9164\n",
      "Epoch 58/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1928 - accuracy: 0.8981 - val_loss: 0.1684 - val_accuracy: 0.9164\n",
      "Epoch 59/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1929 - accuracy: 0.8981 - val_loss: 0.1688 - val_accuracy: 0.9164\n",
      "Epoch 60/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1933 - accuracy: 0.8981 - val_loss: 0.1693 - val_accuracy: 0.9164\n",
      "Epoch 61/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1936 - accuracy: 0.8981 - val_loss: 0.1695 - val_accuracy: 0.9164\n",
      "Epoch 62/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1938 - accuracy: 0.8981 - val_loss: 0.1697 - val_accuracy: 0.9164\n",
      "Epoch 63/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1940 - accuracy: 0.8981 - val_loss: 0.1699 - val_accuracy: 0.9164\n",
      "Epoch 64/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1941 - accuracy: 0.8981 - val_loss: 0.1701 - val_accuracy: 0.9164\n",
      "Epoch 65/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1944 - accuracy: 0.8981 - val_loss: 0.1704 - val_accuracy: 0.9164\n",
      "Epoch 66/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1945 - accuracy: 0.8981 - val_loss: 0.1708 - val_accuracy: 0.9164\n",
      "Epoch 67/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1945 - accuracy: 0.8981 - val_loss: 0.1708 - val_accuracy: 0.9164\n",
      "Epoch 68/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1949 - accuracy: 0.8981 - val_loss: 0.1717 - val_accuracy: 0.9164\n",
      "Epoch 69/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1952 - accuracy: 0.8981 - val_loss: 0.1723 - val_accuracy: 0.9164\n",
      "Epoch 70/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1955 - accuracy: 0.8981 - val_loss: 0.1729 - val_accuracy: 0.9164\n",
      "Epoch 71/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1959 - accuracy: 0.8981 - val_loss: 0.1734 - val_accuracy: 0.9164\n",
      "Epoch 72/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1962 - accuracy: 0.8981 - val_loss: 0.1739 - val_accuracy: 0.9164\n",
      "Epoch 73/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1966 - accuracy: 0.8981 - val_loss: 0.1747 - val_accuracy: 0.9164\n",
      "Epoch 74/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1970 - accuracy: 0.8981 - val_loss: 0.1753 - val_accuracy: 0.9164\n",
      "Epoch 75/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1975 - accuracy: 0.8981 - val_loss: 0.1757 - val_accuracy: 0.9164\n",
      "Epoch 76/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1973 - accuracy: 0.8981 - val_loss: 0.1760 - val_accuracy: 0.9164\n",
      "Epoch 77/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1980 - accuracy: 0.8981 - val_loss: 0.1766 - val_accuracy: 0.9164\n",
      "Epoch 78/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1980 - accuracy: 0.8981 - val_loss: 0.1766 - val_accuracy: 0.9164\n",
      "Epoch 79/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1978 - accuracy: 0.8981 - val_loss: 0.1764 - val_accuracy: 0.9164\n",
      "Epoch 80/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1980 - accuracy: 0.8981 - val_loss: 0.1765 - val_accuracy: 0.9164\n",
      "Epoch 81/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1978 - accuracy: 0.8981 - val_loss: 0.1766 - val_accuracy: 0.9164\n",
      "Epoch 82/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1978 - accuracy: 0.8981 - val_loss: 0.1767 - val_accuracy: 0.9164\n",
      "Epoch 83/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1979 - accuracy: 0.8981 - val_loss: 0.1767 - val_accuracy: 0.9164\n",
      "Epoch 84/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1980 - accuracy: 0.8981 - val_loss: 0.1768 - val_accuracy: 0.9164\n",
      "Epoch 85/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1976 - accuracy: 0.8981 - val_loss: 0.1765 - val_accuracy: 0.9164\n",
      "Epoch 86/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1977 - accuracy: 0.8981 - val_loss: 0.1766 - val_accuracy: 0.9164\n",
      "Epoch 87/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1976 - accuracy: 0.8981 - val_loss: 0.1764 - val_accuracy: 0.9164\n",
      "Epoch 88/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1976 - accuracy: 0.8981 - val_loss: 0.1766 - val_accuracy: 0.9164\n",
      "Epoch 89/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1975 - accuracy: 0.8981 - val_loss: 0.1762 - val_accuracy: 0.9164\n",
      "Epoch 90/100\n",
      "93/93 [==============================] - 0s 3ms/step - loss: 0.1976 - accuracy: 0.8981 - val_loss: 0.1764 - val_accuracy: 0.9164\n",
      "Epoch 91/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1975 - accuracy: 0.8981 - val_loss: 0.1765 - val_accuracy: 0.9164\n",
      "Epoch 92/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1974 - accuracy: 0.8981 - val_loss: 0.1763 - val_accuracy: 0.9164\n",
      "Epoch 93/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1973 - accuracy: 0.8981 - val_loss: 0.1763 - val_accuracy: 0.9164\n",
      "Epoch 94/100\n",
      "93/93 [==============================] - 0s 3ms/step - loss: 0.1973 - accuracy: 0.8981 - val_loss: 0.1763 - val_accuracy: 0.9164\n",
      "Epoch 95/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1973 - accuracy: 0.8981 - val_loss: 0.1762 - val_accuracy: 0.9164\n",
      "Epoch 96/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1969 - accuracy: 0.8981 - val_loss: 0.1761 - val_accuracy: 0.9164\n",
      "Epoch 97/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1974 - accuracy: 0.8981 - val_loss: 0.1759 - val_accuracy: 0.9164\n",
      "Epoch 98/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1972 - accuracy: 0.8981 - val_loss: 0.1761 - val_accuracy: 0.9164\n",
      "Epoch 99/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1969 - accuracy: 0.8981 - val_loss: 0.1760 - val_accuracy: 0.9164\n",
      "Epoch 100/100\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1970 - accuracy: 0.8981 - val_loss: 0.1763 - val_accuracy: 0.9164\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x14b110400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "Test loss: 17.63% \t Test accuracy: 91.64%\n",
      "[[0.6690666  0.20657355 0.10133777 0.02302209]\n",
      " [0.26454118 0.25722992 0.2544621  0.22376674]\n",
      " [0.2645075  0.25722075 0.254469   0.2238027 ]]\n"
     ]
    }
   ],
   "source": [
    "print(nn_model(X_train, Y_train, X_test, Y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
